{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-30T05:27:57.252602Z",
     "iopub.status.busy": "2022-11-30T05:27:57.251483Z",
     "iopub.status.idle": "2022-11-30T05:27:58.064097Z",
     "shell.execute_reply": "2022-11-30T05:27:58.063035Z",
     "shell.execute_reply.started": "2022-11-30T05:27:57.252557Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.transform import *\n",
    "from skimage.color import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:27:58.066764Z",
     "iopub.status.busy": "2022-11-30T05:27:58.066237Z",
     "iopub.status.idle": "2022-11-30T05:29:38.729642Z",
     "shell.execute_reply": "2022-11-30T05:29:38.728672Z",
     "shell.execute_reply.started": "2022-11-30T05:27:58.066722Z"
    }
   },
   "outputs": [],
   "source": [
    "FILE_PATH = '/kaggle/input/petfinder-pawpularity-score/train'\n",
    "\n",
    "count = 0\n",
    "mappings = dict()\n",
    "images = []\n",
    "# parse through all files in the directory\n",
    "for dirpath, dirs, files in os.walk(FILE_PATH): \n",
    "    for filename in files:\n",
    "        if count == 250:\n",
    "            break\n",
    "        img_path = dirpath+'/'+filename\n",
    "        # read images\n",
    "        img = cv2.imread(img_path)\n",
    "        img = resize(img, (32, 32))\n",
    "        mappings[filename.split('.')[0]] = img\n",
    "        images.append(img)\n",
    "        count += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:38.731314Z",
     "iopub.status.busy": "2022-11-30T05:29:38.730948Z",
     "iopub.status.idle": "2022-11-30T05:29:38.745415Z",
     "shell.execute_reply": "2022-11-30T05:29:38.744358Z",
     "shell.execute_reply.started": "2022-11-30T05:29:38.731280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(images).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:38.749641Z",
     "iopub.status.busy": "2022-11-30T05:29:38.747482Z",
     "iopub.status.idle": "2022-11-30T05:29:40.016493Z",
     "shell.execute_reply": "2022-11-30T05:29:40.015373Z",
     "shell.execute_reply.started": "2022-11-30T05:29:38.749604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7954ebb5c90d9618e34959df0ad5f062</td>\n",
       "      <td>[[[0.8085110995462915, 0.8128895632570354, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2969162fab1d0e5a65e4ce02db267745</td>\n",
       "      <td>[[[0.1428951739514301, 0.1937137382307242, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9f32ae94d7723414cb8fa881a1c6626c</td>\n",
       "      <td>[[[0.060462790072490086, 0.0627508977950898, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b8f920f44800ee71e4659dea84bc9bef</td>\n",
       "      <td>[[[0.6358429081863435, 0.6643227203623732, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7fc71b8da143721939715b1cfe22122f</td>\n",
       "      <td>[[[0.08754420143711472, 0.30043968625610373, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  \\\n",
       "0  7954ebb5c90d9618e34959df0ad5f062   \n",
       "1  2969162fab1d0e5a65e4ce02db267745   \n",
       "2  9f32ae94d7723414cb8fa881a1c6626c   \n",
       "3  b8f920f44800ee71e4659dea84bc9bef   \n",
       "4  7fc71b8da143721939715b1cfe22122f   \n",
       "\n",
       "                                               image  \n",
       "0  [[[0.8085110995462915, 0.8128895632570354, 0.8...  \n",
       "1  [[[0.1428951739514301, 0.1937137382307242, 0.2...  \n",
       "2  [[[0.060462790072490086, 0.0627508977950898, 0...  \n",
       "3  [[[0.6358429081863435, 0.6643227203623732, 0.6...  \n",
       "4  [[[0.08754420143711472, 0.30043968625610373, 0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df = pd.DataFrame(mappings.items(), columns = ['Id','image'])\n",
    "images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:40.018131Z",
     "iopub.status.busy": "2022-11-30T05:29:40.017739Z",
     "iopub.status.idle": "2022-11-30T05:29:40.048084Z",
     "shell.execute_reply": "2022-11-30T05:29:40.047167Z",
     "shell.execute_reply.started": "2022-11-30T05:29:40.018086Z"
    }
   },
   "outputs": [],
   "source": [
    "train_table = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:40.050051Z",
     "iopub.status.busy": "2022-11-30T05:29:40.049679Z",
     "iopub.status.idle": "2022-11-30T05:29:40.063116Z",
     "shell.execute_reply": "2022-11-30T05:29:40.062071Z",
     "shell.execute_reply.started": "2022-11-30T05:29:40.050016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:40.065413Z",
     "iopub.status.busy": "2022-11-30T05:29:40.064723Z",
     "iopub.status.idle": "2022-11-30T05:29:40.091277Z",
     "shell.execute_reply": "2022-11-30T05:29:40.090500Z",
     "shell.execute_reply.started": "2022-11-30T05:29:40.065362Z"
    }
   },
   "outputs": [],
   "source": [
    "paw_merge = images_df.merge(train_table, on = 'Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:40.093092Z",
     "iopub.status.busy": "2022-11-30T05:29:40.092689Z",
     "iopub.status.idle": "2022-11-30T05:29:41.356605Z",
     "shell.execute_reply": "2022-11-30T05:29:41.355722Z",
     "shell.execute_reply.started": "2022-11-30T05:29:40.093057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.8085110995462915, 0.8128895632570354, 0.8...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[0.1428951739514301, 0.1937137382307242, 0.2...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0.060462790072490086, 0.0627508977950898, 0...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[0.6358429081863435, 0.6643227203623732, 0.6...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[0.08754420143711472, 0.30043968625610373, 0...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  Pawpularity\n",
       "0  [[[0.8085110995462915, 0.8128895632570354, 0.8...           38\n",
       "1  [[[0.1428951739514301, 0.1937137382307242, 0.2...           29\n",
       "2  [[[0.060462790072490086, 0.0627508977950898, 0...           26\n",
       "3  [[[0.6358429081863435, 0.6643227203623732, 0.6...           27\n",
       "4  [[[0.08754420143711472, 0.30043968625610373, 0...           19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_score_df = paw_merge[['image', 'Pawpularity']]\n",
    "image_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.359896Z",
     "iopub.status.busy": "2022-11-30T05:29:41.359562Z",
     "iopub.status.idle": "2022-11-30T05:29:41.365171Z",
     "shell.execute_reply": "2022-11-30T05:29:41.363862Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.359868Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = image_score_df.iloc[0:200, :]\n",
    "valid_df = image_score_df.iloc[200:225, :]\n",
    "test_df = image_score_df.iloc[225:250, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.371289Z",
     "iopub.status.busy": "2022-11-30T05:29:41.371016Z",
     "iopub.status.idle": "2022-11-30T05:29:41.378151Z",
     "shell.execute_reply": "2022-11-30T05:29:41.377069Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.371251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n",
      "(25, 2)\n",
      "(25, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(valid_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.380745Z",
     "iopub.status.busy": "2022-11-30T05:29:41.379890Z",
     "iopub.status.idle": "2022-11-30T05:29:41.396179Z",
     "shell.execute_reply": "2022-11-30T05:29:41.395328Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.380708Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array(np.array(images[0:200]) * 255, dtype = np.uint8)\n",
    "y_train = train_df['Pawpularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.397828Z",
     "iopub.status.busy": "2022-11-30T05:29:41.397418Z",
     "iopub.status.idle": "2022-11-30T05:29:41.404654Z",
     "shell.execute_reply": "2022-11-30T05:29:41.403436Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.397792Z"
    }
   },
   "outputs": [],
   "source": [
    "x_val = np.array(np.array(images[200:225]) * 255, dtype = np.uint8)\n",
    "y_val = valid_df['Pawpularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.407271Z",
     "iopub.status.busy": "2022-11-30T05:29:41.406102Z",
     "iopub.status.idle": "2022-11-30T05:29:41.415898Z",
     "shell.execute_reply": "2022-11-30T05:29:41.414890Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.407236Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.array(np.array(images[225:250]) * 255, dtype = np.uint8)\n",
    "y_test = test_df['Pawpularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.418182Z",
     "iopub.status.busy": "2022-11-30T05:29:41.417640Z",
     "iopub.status.idle": "2022-11-30T05:29:41.427195Z",
     "shell.execute_reply": "2022-11-30T05:29:41.425940Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.418146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 32, 32, 3)\n",
      "(200,)\n",
      "(25, 32, 32, 3)\n",
      "(25,)\n",
      "(25, 32, 32, 3)\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:41.429588Z",
     "iopub.status.busy": "2022-11-30T05:29:41.429211Z",
     "iopub.status.idle": "2022-11-30T05:29:42.383857Z",
     "shell.execute_reply": "2022-11-30T05:29:42.382793Z",
     "shell.execute_reply.started": "2022-11-30T05:29:41.429551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip install -U tensorflow-addons: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:42.387754Z",
     "iopub.status.busy": "2022-11-30T05:29:42.387441Z",
     "iopub.status.idle": "2022-11-30T05:29:48.592807Z",
     "shell.execute_reply": "2022-11-30T05:29:48.591840Z",
     "shell.execute_reply.started": "2022-11-30T05:29:42.387722Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Reshape\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation, RandomContrast\n",
    "\n",
    "# Set seed for reproducibiltiy\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:48.595094Z",
     "iopub.status.busy": "2022-11-30T05:29:48.594464Z",
     "iopub.status.idle": "2022-11-30T05:29:48.607625Z",
     "shell.execute_reply": "2022-11-30T05:29:48.603831Z",
     "shell.execute_reply.started": "2022-11-30T05:29:48.595057Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = BATCH_SIZE * 2\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "NUM_CLASSES = 1  # for attention mapping related to continuous regression decision\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 32  # We will resize input images to this size.\n",
    "\n",
    "# ARCHITECTURE\n",
    "DIMENSIONS = 3072\n",
    "SE_RATIO = 8\n",
    "TRUNK_DEPTH = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:48.611705Z",
     "iopub.status.busy": "2022-11-30T05:29:48.611054Z",
     "iopub.status.idle": "2022-11-30T05:29:52.443238Z",
     "shell.execute_reply": "2022-11-30T05:29:52.442087Z",
     "shell.execute_reply.started": "2022-11-30T05:29:48.611668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 200\n",
      "Validation samples: 25\n",
      "Testing samples: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 05:29:48.731654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.732611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.912033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.912918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.913695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.914462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:48.916665: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 05:29:49.167952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:49.168850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:49.169589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:49.170293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:49.170993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:49.171681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.069858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.070787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.071525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.072233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.072936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.073594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-11-30 05:29:52.076818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-30 05:29:52.077550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:52.445268Z",
     "iopub.status.busy": "2022-11-30T05:29:52.444637Z",
     "iopub.status.idle": "2022-11-30T05:29:52.453803Z",
     "shell.execute_reply": "2022-11-30T05:29:52.452633Z",
     "shell.execute_reply.started": "2022-11-30T05:29:52.445231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 32, 32, 3), (None,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:52.455827Z",
     "iopub.status.busy": "2022-11-30T05:29:52.455334Z",
     "iopub.status.idle": "2022-11-30T05:29:52.463825Z",
     "shell.execute_reply": "2022-11-30T05:29:52.462618Z",
     "shell.execute_reply.started": "2022-11-30T05:29:52.455790Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessing():\n",
    "    model = keras.Sequential(\n",
    "        [layers.Rescaling(1 / 255.0), layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n",
    "        name=\"preprocessing\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_train_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255.0),\n",
    "            layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "            layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "        ],\n",
    "        name=\"train_data_augmentation\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:52.465953Z",
     "iopub.status.busy": "2022-11-30T05:29:52.465255Z",
     "iopub.status.idle": "2022-11-30T05:29:52.475424Z",
     "shell.execute_reply": "2022-11-30T05:29:52.474184Z",
     "shell.execute_reply.started": "2022-11-30T05:29:52.465919Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_convolutional_stem(dimensions):\n",
    "    \"\"\"Build the convolutional stem.\n",
    "\n",
    "    Args:\n",
    "        dimensions: The embedding dimension of the patches (d in paper).\n",
    "\n",
    "    Returs:\n",
    "        The convolutional stem as a keras seqeuntial\n",
    "        model.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"kernel_size\": (3, 3),\n",
    "        \"strides\": (2, 2),\n",
    "        \"activation\": tf.nn.gelu,\n",
    "        \"padding\": \"same\",\n",
    "    }\n",
    "\n",
    "    convolutional_stem = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(filters=dimensions // 2, **config),\n",
    "            layers.Conv2D(filters=dimensions, **config),\n",
    "        ],\n",
    "        name=\"convolutional_stem\",\n",
    "    )\n",
    "\n",
    "    return convolutional_stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:29:52.478710Z",
     "iopub.status.busy": "2022-11-30T05:29:52.477915Z",
     "iopub.status.idle": "2022-11-30T05:29:52.494366Z",
     "shell.execute_reply": "2022-11-30T05:29:52.493381Z",
     "shell.execute_reply.started": "2022-11-30T05:29:52.478653Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class SqueezeExcite(layers.Layer):\n",
    "    \"\"\"Applies squeeze and excitation to input feature maps as seen in\n",
    "    https://arxiv.org/abs/1709.01507.\n",
    "\n",
    "    Args:\n",
    "        ratio: The ratio with which the feature map needs to be reduced in\n",
    "        the reduction phase.\n",
    "\n",
    "    Inputs:\n",
    "        Convolutional features.\n",
    "\n",
    "    Outputs:\n",
    "        Attention modified feature maps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"ratio\": self.ratio})\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        filters = input_shape[-1]\n",
    "        self.squeeze = layers.GlobalAveragePooling2D(keepdims=True)\n",
    "        self.reduction = layers.Dense(\n",
    "            units=filters // self.ratio, activation=\"relu\", use_bias=False,\n",
    "        )\n",
    "        self.excite = layers.Dense(units=filters, activation=\"sigmoid\", use_bias=False)\n",
    "        self.multiply = layers.Multiply()\n",
    "\n",
    "    def call(self, x):\n",
    "        shortcut = x\n",
    "        x = self.squeeze(x)\n",
    "        x = self.reduction(x)\n",
    "        x = self.excite(x)\n",
    "        x = self.multiply([shortcut, x])\n",
    "        return x\n",
    "\n",
    "\n",
    "class Trunk(layers.Layer):\n",
    "    \"\"\"Convolutional residual trunk as in the https://arxiv.org/abs/2112.13692\n",
    "\n",
    "    Args:\n",
    "        depth: Number of trunk residual blocks\n",
    "        dimensions: Dimnesion of the model (denoted by d in the paper)\n",
    "        ratio: The Squeeze-Excitation ratio\n",
    "\n",
    "    Inputs:\n",
    "        Convolutional features extracted from the conv stem.\n",
    "\n",
    "    Outputs:\n",
    "        Flattened patches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, dimensions, ratio, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "        self.dimensions = dimensions\n",
    "        self.depth = depth\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\"ratio\": self.ratio, \"dimensions\": self.dimensions, \"depth\": self.depth,}\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        config = {\n",
    "            \"filters\": self.dimensions,\n",
    "            \"activation\": tf.nn.gelu,\n",
    "            \"padding\": \"same\",\n",
    "        }\n",
    "\n",
    "        trunk_block = [\n",
    "            layers.LayerNormalization(epsilon=1e-6),\n",
    "            layers.Conv2D(kernel_size=(1, 1), **config),\n",
    "            layers.Conv2D(kernel_size=(3, 3), **config),\n",
    "            SqueezeExcite(ratio=self.ratio),\n",
    "            layers.Conv2D(kernel_size=(1, 1), filters=self.dimensions, padding=\"same\"),\n",
    "        ]\n",
    "\n",
    "        self.trunk_blocks = [keras.Sequential(trunk_block) for _ in range(self.depth)]\n",
    "        self.add = layers.Add()\n",
    "        self.flatten_spatial = layers.Reshape((-1, self.dimensions))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Remember the input.\n",
    "        shortcut = x\n",
    "        for trunk_block in self.trunk_blocks:\n",
    "            output = trunk_block(x)\n",
    "            shortcut = self.add([output, shortcut])\n",
    "            x = shortcut\n",
    "        # Flatten the patches.\n",
    "        x = self.flatten_spatial(x)\n",
    "        print(\"truck dimen {}\".format(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:41:57.228422Z",
     "iopub.status.busy": "2022-11-30T05:41:57.227327Z",
     "iopub.status.idle": "2022-11-30T05:41:57.244675Z",
     "shell.execute_reply": "2022-11-30T05:41:57.243661Z",
     "shell.execute_reply.started": "2022-11-30T05:41:57.228354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taking a batch of test inputs to measure model's progress.\n",
    "test_images, test_labels = next(iter(test_ds))\n",
    "\n",
    "\n",
    "class TrainMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.epoch_interval and epoch % self.epoch_interval == 4:\n",
    "            test_augmented_images = self.model.preprocessing_model(test_images)\n",
    "            # Pass through the stem.\n",
    "            test_x = self.model.stem(test_augmented_images)\n",
    "            # Pass through the trunk.\n",
    "            test_x = self.model.trunk(test_x)\n",
    "            # Pass through the attention pooling block.\n",
    "            _, test_viz_weights = self.model.attention_pooling(test_x)\n",
    "            # Reshape the vizualization weights\n",
    "            num_patches = tf.shape(test_viz_weights)[-1]\n",
    "            #print(num_patches)\n",
    "            height = width = int(math.sqrt(num_patches))\n",
    "            test_viz_weights = layers.Reshape((height, width))(test_viz_weights)\n",
    "            # Take a random image and its attention weights.\n",
    "            index = np.random.randint(low=0, high=tf.shape(test_augmented_images)[0])\n",
    "            selected_image = test_augmented_images[index]\n",
    "            selected_weight = test_viz_weights[index]\n",
    "            #print(sum(selected_weight))\n",
    "            # Plot the images and the overlayed attention map.\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "            ax[0].imshow(selected_image)\n",
    "            ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "            ax[0].axis(\"off\")\n",
    "            img = ax[1].imshow(selected_image)\n",
    "            ax[1].imshow(\n",
    "                selected_weight, cmap=\"inferno\", alpha=0.6, extent=img.get_extent()\n",
    "            )\n",
    "            ax[1].set_title(f\"Attended: {epoch:03d}\")\n",
    "            ax[1].axis(\"off\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:41:58.486187Z",
     "iopub.status.busy": "2022-11-30T05:41:58.485828Z",
     "iopub.status.idle": "2022-11-30T05:41:58.497242Z",
     "shell.execute_reply": "2022-11-30T05:41:58.496094Z",
     "shell.execute_reply.started": "2022-11-30T05:41:58.486156Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "\n",
    "total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "warmup_epoch_percentage = 0.15\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=LEARNING_RATE,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:42:01.732357Z",
     "iopub.status.busy": "2022-11-30T05:42:01.731990Z",
     "iopub.status.idle": "2022-11-30T05:42:01.746655Z",
     "shell.execute_reply": "2022-11-30T05:42:01.745418Z",
     "shell.execute_reply.started": "2022-11-30T05:42:01.732324Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class AttentionPooling(layers.Layer):\n",
    "    \"\"\"Applies attention to the patches extracted form the\n",
    "    trunk with the CLS token.\n",
    "\n",
    "    Args:\n",
    "        dimensions: The dimension of the whole architecture.\n",
    "        num_classes: The number of classes in the dataset.\n",
    "\n",
    "    Inputs:\n",
    "        Flattened patches from the trunk.\n",
    "\n",
    "    Outputs:\n",
    "        The modified CLS token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dimensions = dimensions\n",
    "        self.num_classes = num_classes\n",
    "        self.cls = tf.Variable(tf.zeros((1, 1, dimensions)))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"dimensions\": self.dimensions,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"cls\": self.cls.numpy(),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=1, key_dim=self.dimensions, dropout=0.2,\n",
    "        )\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=self.dimensions, activation=tf.nn.gelu),\n",
    "                layers.Dropout(0.2),\n",
    "                layers.Dense(units=self.dimensions, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        #Final layer of attention map\n",
    "        self.dense = layers.Dense(units=1,activation='linear')\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.global2d = layers.GlobalAveragePooling2D()\n",
    "        self.reshape = layers.Reshape(target_shape=(32,32,3))\n",
    "        self.resnet = ResNet50(include_top=False, weights= 'imagenet', pooling=None, input_shape = (32, 32 , 3))\n",
    "        \n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        #print(\"Shape of one patch: {}\" .format(tf.shape(x)))\n",
    "        # Expand the class token batch number of times.\n",
    "        class_token = tf.repeat(self.cls, repeats=batch_size, axis=0)\n",
    "        # Concat the input with the trainable class token.\n",
    "        x = tf.concat([class_token, x], axis=1)\n",
    "        # Apply attention to x.\n",
    "        x = self.layer_norm1(x)\n",
    "        x, viz_weights = self.attention(\n",
    "            query=x[:, 0:1], key=x, value=x, return_attention_scores=True\n",
    "        )\n",
    "        \n",
    "        #print(viz_weights)\n",
    "        #print(x)\n",
    "        \n",
    "        class_token = class_token + x\n",
    "        \n",
    "        #print(class_token)\n",
    "        \n",
    "        class_token = self.layer_norm2(class_token)\n",
    "        class_token = self.flatten(class_token)\n",
    "        class_token = self.layer_norm3(class_token)\n",
    "        class_token = class_token + self.mlp(class_token)\n",
    "        \n",
    "        class_token = self.reshape(class_token)\n",
    "        \n",
    "        #256 1D\n",
    "        #print(class_token)\n",
    "        \n",
    "        #print(image_inputs)\n",
    "        \n",
    "        image_x = self.resnet(class_token)\n",
    "        #print(image_x)\n",
    "        \n",
    "        image_x = self.global2d(image_x)\n",
    "        \n",
    "        #print(image_x)\n",
    "        # Build the logits\n",
    "        logits = self.dense(image_x)\n",
    "        #print(logits)\n",
    "        \n",
    "        return logits, tf.squeeze(viz_weights)[..., 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:42:03.247626Z",
     "iopub.status.busy": "2022-11-30T05:42:03.247236Z",
     "iopub.status.idle": "2022-11-30T05:42:03.261944Z",
     "shell.execute_reply": "2022-11-30T05:42:03.260674Z",
     "shell.execute_reply.started": "2022-11-30T05:42:03.247591Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchConvNet(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stem,\n",
    "        trunk,\n",
    "        attention_pooling,\n",
    "        preprocessing_model,\n",
    "        train_augmentation_model,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stem = stem\n",
    "        self.trunk = trunk\n",
    "        self.attention_pooling = attention_pooling\n",
    "        self.train_augmentation_model = train_augmentation_model\n",
    "        self.preprocessing_model = preprocessing_model\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"stem\": self.stem,\n",
    "                \"trunk\": self.trunk,\n",
    "                \"attention_pooling\": self.attention_pooling,\n",
    "                \"train_augmentation_model\": self.train_augmentation_model,\n",
    "                \"preprocessing_model\": self.preprocessing_model,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def _calculate_loss(self, inputs, test=False):\n",
    "        images, labels = inputs\n",
    "        # Augment the input images.\n",
    "        if test:\n",
    "            augmented_images = self.preprocessing_model(images)\n",
    "        else:\n",
    "            augmented_images = self.train_augmentation_model(images)\n",
    "        # Pass through the stem.\n",
    "        x = self.stem(augmented_images)\n",
    "        # Pass through the trunk.\n",
    "        x = self.trunk(x)\n",
    "        # Pass through the attention pooling block.\n",
    "        #print('self trunk'.format(x))\n",
    "        logits, _ = self.attention_pooling(x)\n",
    "        # Compute the total loss.\n",
    "        #print(logits)\n",
    "        total_loss = self.compiled_loss(labels, logits)\n",
    "        #print(total_loss)\n",
    "        return total_loss, logits\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, logits = self._calculate_loss(inputs)\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.stem.trainable_variables,\n",
    "            self.trunk.trainable_variables,\n",
    "            self.attention_pooling.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        trainable_variable_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                trainable_variable_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(trainable_variable_list)\n",
    "        # Report progress.\n",
    "        _, labels = inputs\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        total_loss, logits = self._calculate_loss(inputs, test=True)\n",
    "        # Report progress.\n",
    "        _, labels = inputs\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, images):\n",
    "        # Augment the input images.\n",
    "        augmented_images = self.preprocessing_model(images)\n",
    "        # Pass through the stem.\n",
    "        x = self.stem(augmented_images)\n",
    "        # Pass through the trunk.\n",
    "        x = self.trunk(x)\n",
    "        # Pass through the attention pooling block.\n",
    "        logits, viz_weights = self.attention_pooling(x)\n",
    "        return logits, viz_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:42:04.931914Z",
     "iopub.status.busy": "2022-11-30T05:42:04.931552Z",
     "iopub.status.idle": "2022-11-30T05:42:07.186745Z",
     "shell.execute_reply": "2022-11-30T05:42:07.184311Z",
     "shell.execute_reply.started": "2022-11-30T05:42:04.931885Z"
    }
   },
   "outputs": [],
   "source": [
    "train_augmentation_model = get_train_augmentation_model()\n",
    "preprocessing_model = get_preprocessing()\n",
    "conv_stem = build_convolutional_stem(dimensions=DIMENSIONS)\n",
    "conv_trunk = Trunk(depth=TRUNK_DEPTH, dimensions=DIMENSIONS, ratio=SE_RATIO)\n",
    "attention_pooling = AttentionPooling(dimensions=DIMENSIONS, num_classes=NUM_CLASSES)\n",
    "\n",
    "patch_conv_net = PatchConvNet(\n",
    "    stem=conv_stem,\n",
    "    trunk=conv_trunk,\n",
    "    attention_pooling=attention_pooling,\n",
    "    train_augmentation_model=train_augmentation_model,\n",
    "    preprocessing_model=preprocessing_model,\n",
    ")\n",
    "\n",
    "# Assemble the callbacks.\n",
    "train_callbacks = [TrainMonitor(epoch_interval=5)]\n",
    "# Get the optimizer.\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=scheduled_lrs, weight_decay=WEIGHT_DECAY)\n",
    "# Compile and pretrain the model.\n",
    "patch_conv_net.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    metrics=[\n",
    "        keras.metrics.RootMeanSquaredError(name=\"root_mean_squared_error\", dtype=None\n",
    "),\n",
    "    ],\n",
    ")\n",
    "history = patch_conv_net.fit(\n",
    "    train_ds, epochs=EPOCHS,\n",
    "     validation_data=val_ds,\n",
    "      callbacks=train_callbacks,\n",
    ")\n",
    "\n",
    "# Evaluate the model with the test dataset.\n",
    "loss, rmse = patch_conv_net.evaluate(test_ds)\n",
    "print(f\"Loss: {loss:0.2f}\")\n",
    "print(f\"RMSE: {rmse:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-30T05:45:52.483887Z",
     "iopub.status.busy": "2022-11-30T05:45:52.483442Z",
     "iopub.status.idle": "2022-11-30T05:45:52.929367Z",
     "shell.execute_reply": "2022-11-30T05:45:52.928450Z",
     "shell.execute_reply.started": "2022-11-30T05:45:52.483853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck dimen [[[-0.21346168  0.0702888   0.11949164 ...  0.137703   -0.02793177\n",
      "   -0.02564799]\n",
      "  [-0.1412999   0.18183282 -0.15954071 ...  0.0417731  -0.08258563\n",
      "   -0.15251026]\n",
      "  [-0.18583393  0.35098535 -0.35342532 ... -0.16652568 -0.04940495\n",
      "   -0.1645639 ]\n",
      "  ...\n",
      "  [-0.1895305   0.09126476 -0.09443606 ...  0.05756614  0.07471318\n",
      "   -0.17443801]\n",
      "  [-0.26490933  0.08607281 -0.14232472 ...  0.01172609  0.17047475\n",
      "   -0.11429042]\n",
      "  [ 0.00612728 -0.08253981  0.05925277 ...  0.0221121   0.01582574\n",
      "   -0.01816319]]]\n",
      "viz_weights first (144,)\n",
      "viz_weights (1, 144)\n",
      "viz_weights reshaped (1, 12, 12)\n",
      "(1, 12, 12)\n",
      "selected image shape (48, 48, 3)\n",
      "selected weight (12, 12)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_attention(image):\n",
    "    \"\"\"Plots the attention map on top of the image.\n",
    "\n",
    "    Args:\n",
    "        image: A numpy image of arbitrary size.\n",
    "    \"\"\"\n",
    "    # Resize the image to a (32, 32) dim.\n",
    "    #image = tf.image.resize(image, (32, 32))\n",
    "    image = image[tf.newaxis, ...]\n",
    "    test_augmented_images = patch_conv_net.preprocessing_model(image)\n",
    "    #print(test_augmented_images.shape)\n",
    "    # Pass through the stem.\n",
    "    test_x = patch_conv_net.stem(test_augmented_images)\n",
    "    #print(test_x.shape)\n",
    "    # Pass through the trunk.\n",
    "    test_x = patch_conv_net.trunk(test_x)\n",
    "    #print(test_x.shape)\n",
    "    # Pass through the attention pooling block.\n",
    "    _, test_viz_weights = patch_conv_net.attention_pooling(test_x)\n",
    "    print(\"viz_weights first\", test_viz_weights.shape)\n",
    "    test_viz_weights = test_viz_weights[tf.newaxis, ...]\n",
    "    print(\"viz_weights\", test_viz_weights.shape)\n",
    "    # Reshape the vizualization weights.\n",
    "    num_patches = tf.shape(test_viz_weights)[-1]\n",
    "    #print(num_patches)\n",
    "    height = width = int(math.sqrt(num_patches))\n",
    "    test_viz_weights = layers.Reshape((height, width))(test_viz_weights)\n",
    "    print(\"viz_weights reshaped\", test_viz_weights.shape)\n",
    "    print(test_viz_weights.shape)\n",
    "    selected_image = test_augmented_images[0]\n",
    "    print(\"selected image shape\",selected_image.shape)\n",
    "    selected_weight = test_viz_weights[0]\n",
    "    print(\"selected weight\",selected_weight.shape)\n",
    "    #print(selected_weight.shape)\n",
    "    # Plot the images.\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax[0].imshow(selected_image)\n",
    "    ax[0].set_title(f\"Original\")\n",
    "    ax[0].axis(\"off\")\n",
    "    img = ax[1].imshow(selected_image)\n",
    "    ax[1].imshow(selected_weight, cmap=\"inferno\", alpha=0.6, extent=img.get_extent())\n",
    "    ax[1].set_title(f\"Attended\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# url = \"http://farm9.staticflickr.com/8017/7140384795_385b1f48df_z.jpg\"\n",
    "# image_name = keras.utils.get_file(fname=\"image.jpg\", origin=url)\n",
    "# image = tf.io.read_file(image_name)\n",
    "# image = tf.io.decode_image(image)\n",
    "#plot_attention(np.array(aval_x_test[0], dtype = np.uint8))\n",
    "#plot_attention(np.array(aval_x_test[0], dtype = np.uint8))\n",
    "\n",
    "plot_attention(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One sample example is shown here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
